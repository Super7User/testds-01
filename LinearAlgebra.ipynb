{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1727475d-aa8e-49de-90e9-a8af92adb370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.5.0)\n",
      "Collecting sympy (from torch)\n",
      "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.7.101 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch)\n",
      "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch)\n",
      "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch)\n",
      "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91 (from torch)\n",
      "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.0.1 (from torch)\n",
      "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91 (from torch)\n",
      "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu11==2.14.3 (from torch)\n",
      "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91 (from torch)\n",
      "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m269.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting triton==2.0.0 (from torch)\n",
      "  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (67.7.2)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.40.0)\n",
      "Collecting cmake (from triton==2.0.0->torch)\n",
      "  Downloading cmake-3.26.3-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (24.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.0/24.0 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting lit (from triton==2.0.0->torch)\n",
      "  Downloading lit-16.0.5.post0.tar.gz (138 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.1/138.1 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.2)\n",
      "Collecting mpmath>=0.19 (from sympy->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: lit\n",
      "  Building wheel for lit (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lit: filename=lit-16.0.5.post0-py3-none-any.whl size=88257 sha256=26e05704964b3555e3a4ae1883fc86c0628ae0cc84937a3d3312a41e9451a2e0\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/1a/24/92/1e1c9e37be8411a7c7c18a4c54962f5d0a75c56bab4a6f7f57\n",
      "Successfully built lit\n",
      "Installing collected packages: mpmath, lit, cmake, sympy, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch\n",
      "Successfully installed cmake-3.26.3 lit-16.0.5.post0 mpmath-1.3.0 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 sympy-1.12 torch-2.0.1 triton-2.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17752586-8f45-49af-baad-5b0df3cf1338",
   "metadata": {},
   "source": [
    "Most everyday mathematics consists of manipulating numbers one at a time. Formally, we call these values scalars. For example, the temperature in Palo Alto is a balmy 72 degrees Fahrenheit. If you wanted to convert the temperature to Celsius you would evaluate the expression, setting to c = 5/9*(f-32) In this equation, the values \n",
    "5,9, and 32 are scalars. The variables c and f represent unknown scalars."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe12b8a-2bd0-41cc-aa63-bf7a3672af87",
   "metadata": {},
   "source": [
    "refer scalar from this link http://preview.d2l.ai/d2l-en/master/chapter_preliminaries/linear-algebra.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58722f9c-c93e-4447-8f2e-959a962153a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(5.), tensor(-1.), tensor(6.), tensor(0.6667), tensor(8.))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.tensor(2.0)\n",
    "y = torch.tensor(3.0)\n",
    "x+y, x-y, x*y, x/y, x**y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702a722d-bef5-4b59-b0b1-8a6822a794d2",
   "metadata": {},
   "source": [
    "vectors as fixed-length arrays of scalars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0eff9886-0a76-4311-8b4f-d0aa5039f6ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(5)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f65090-a767-45e8-bfcd-55465630bd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "We can refer to an element of a vector by using a subscript. For example, x subscript 2 denotes the second element of \n",
    "x. Since x subscript 2 is a scalar, we do not bold it. By default, we visualize vectors by stacking their elements vertically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03920be-3612-4ed9-ab4b-2638aae89127",
   "metadata": {},
   "source": [
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\">\n",
    "  <mtable displaystyle=\"true\" columnalign=\"right\" columnspacing=\"0em\" rowspacing=\"3pt\">\n",
    "    <mtr>\n",
    "      <mtd>\n",
    "        <mrow data-mjx-texclass=\"ORD\">\n",
    "          <mi mathvariant=\"bold\">x</mi>\n",
    "        </mrow>\n",
    "        <mo>=</mo>\n",
    "        <mrow data-mjx-texclass=\"INNER\">\n",
    "          <mo data-mjx-texclass=\"OPEN\">[</mo>\n",
    "          <mtable columnalign=\"center\" columnspacing=\"1em\" rowspacing=\"4pt\">\n",
    "            <mtr>\n",
    "              <mtd>\n",
    "                <msub>\n",
    "                  <mi>x</mi>\n",
    "                  <mrow data-mjx-texclass=\"ORD\">\n",
    "                    <mn>1</mn>\n",
    "                  </mrow>\n",
    "                </msub>\n",
    "              </mtd>\n",
    "            </mtr>\n",
    "            <mtr>\n",
    "              <mtd>\n",
    "                <mrow data-mjx-texclass=\"ORD\">\n",
    "                  <mo>&#x22EE;</mo>\n",
    "                </mrow>\n",
    "              </mtd>\n",
    "            </mtr>\n",
    "            <mtr>\n",
    "              <mtd>\n",
    "                <msub>\n",
    "                  <mi>x</mi>\n",
    "                  <mrow data-mjx-texclass=\"ORD\">\n",
    "                    <mi>n</mi>\n",
    "                  </mrow>\n",
    "                </msub>\n",
    "              </mtd>\n",
    "            </mtr>\n",
    "          </mtable>\n",
    "          <mo data-mjx-texclass=\"CLOSE\">]</mo>\n",
    "        </mrow>\n",
    "        <mo>,</mo>\n",
    "      </mtd>\n",
    "    </mtr>\n",
    "  </mtable>\n",
    "</math>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b3d5110-956e-4677-9d32-58820d28f4a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32843b4-21d5-4872-adaa-755319684510",
   "metadata": {},
   "source": [
    "To indicate that a vector contains n elements, we write"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af24f04-554d-408f-98b8-fd928fe5c21e",
   "metadata": {},
   "source": [
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n",
    "  <mrow data-mjx-texclass=\"ORD\">\n",
    "    <mi mathvariant=\"bold\">x</mi>\n",
    "  </mrow>\n",
    "  <mo>&#x2208;</mo>\n",
    "  <msup>\n",
    "    <mrow data-mjx-texclass=\"ORD\">\n",
    "      <mi mathvariant=\"double-struck\">R</mi>\n",
    "    </mrow>\n",
    "    <mi>n</mi>\n",
    "  </msup>\n",
    "</math>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e31d0d-bbae-47bd-8681-b976963ef7fd",
   "metadata": {},
   "source": [
    "Formally, we call n the dimensionality of the vector. In code, this corresponds to the tensor’s length, accessible via Python’s built-in len function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ea8413c-9a52-43da-9ff0-200fd14a85b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facd4013-22c4-4ea2-b989-1629e3589f42",
   "metadata": {},
   "source": [
    "We can also access the length via the shape attribute. The shape is a tuple that indicates a tensor’s length along each axis. Tensors with just one axis have shapes with just one element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fceab98f-63fd-493b-ad09-b60769385bd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8517370-391e-4c85-91cb-64281d540538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [2, 3],\n",
       "        [4, 5]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(6).reshape(3, 2)\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d91184-dd8a-4694-825d-abe060b6ced5",
   "metadata": {},
   "source": [
    "Sometimes, we want to flip the axes. When we exchange a matrix’s rows and columns, the result is called its transpose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8556c88-ccfa-4442-8afb-93824c9af6d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 2, 4],\n",
       "        [1, 3, 5]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4409568f-2db3-46e0-baec-f40af002c805",
   "metadata": {},
   "source": [
    "Symmetric matrices are the subset of square matrices that are equal to their own transposes: \n",
    ". The following matrix is symmetric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97ea3ab3-b81c-408f-9a25-a05d88b8759d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [2, 0, 4],\n",
       "        [3, 4, 5]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.tensor([[1, 2, 3], [2, 0, 4], [3, 4, 5]])\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a321a3a-a614-4d4d-8492-d3e33c767ff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [2, 0, 4],\n",
       "        [3, 4, 5]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7b3bd0db-1329-49f9-8201-f73c42453219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True],\n",
       "        [True, True, True],\n",
       "        [True, True, True]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A == A.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f62991-3d23-4269-b08c-dcc31b58f7bb",
   "metadata": {},
   "source": [
    "While you can go far in your machine learning journey with only scalars, vectors, and matrices, eventually you may need to work with higher-order tensors. Tensors give us a generic way to describe extensions to nth \n",
    "-order arrays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d9a0aba0-1327-4837-a45a-2d8b7ad1c693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3,  4],\n",
       "         [ 5,  6,  7,  8,  9]],\n",
       "\n",
       "        [[10, 11, 12, 13, 14],\n",
       "         [15, 16, 17, 18, 19]],\n",
       "\n",
       "        [[20, 21, 22, 23, 24],\n",
       "         [25, 26, 27, 28, 29]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(30).reshape(3,2,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6d7ff0-b9ea-439b-8abf-1b69107858b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m108"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
